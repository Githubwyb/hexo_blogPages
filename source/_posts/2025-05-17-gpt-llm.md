---
title: llm和gpt学习
date: 2025-05-17 09:10:34
tags: [AI]
categories: [Knowledge, Study]
---

# 一、前言

大模型核心：大算力+强算法+大数据

deepseek部署简单，可以私有化部署

# 应用场景

嵌入：通过prompt及其应用直接调用大模型能力
扩展：通过RAG技术结合企业私域知识增强大模型能力
定制：结合精标注数据进行大模型微调
自建：通过二次训练生成行业垂直大模型

基础设施复杂性大幅提高，运维成本进一步提高

数据质量不高 问答效果不达预期 难定位效果瓶颈，不知道怎么优化


# 云计算和大模型

SaaS：AI应用服务
MaaS： 模型服务
PaaS：AI应用开发平台
IaaS：模型训练、部署
      资源池

训练大模型：垂直领域私有化数据+中文语料库+垂直领域开源数据

# transformer架构

## 1. tokenization

语料库构建BPE词表

原始语句编码，编码可以还原语句

### BPE Byte Pair Encoding 分词：构建词表

## 2. embedding 词元嵌入表示

将token变成高维的向量表示，找到token之间的关联关系

男人和女人关联，篮球运动员和橄榄球运动员

通过距离找关联关系

## 3. word2vec 从大量文本语料中以无监督的学习方式学习语义知识

skip-gram 给定input word来预测上下文
CBOW: 给定上下文，预测input word

## 4. 位置编码

1. 位置不同，词含义不同

## 5. 多头自注意力

将词嵌入向量通过不同的线性层转化为查询Q，键K和值V

## 6. 残差连接

缓解梯度爆炸和消失问题

## 7. 层归一化

## 8. 位置感知前馈网络


# transformer 是什么

1. embedding：先将自然语言转化为计算机可以理解的话，并且表达了一下词之间的关联，关联本身也是参数的一种
2. layer norm：将参数归一化，防止参数差异太大导致训练梯度爆炸，柔和下来
3. self-attention+projection：通过自注意力机制来找到语句里面的重点，提取特征
4. FFN：将语句的重点做一下非线性变换。将self-attention升维，找到里面更深层次的关联和差异，更好的训练
5. 全连接层将输入进行整理输出
6. output：将输出变成概率，用于选择token

# llm应用

## 1. RAG 知识库

## 2. Agent 智能体

1. 记忆模块
2. 规划模块：规划某个问题要做什么工作，如调用工具获取信息
3. 工具模块：调用其他api获取信息，如google、百度、数据库等
