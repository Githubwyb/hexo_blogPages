---
title: 论文翻译——毕设任务
date: 2018-12-27 15:15:49
tags: [reading]
categories: [notes, reading]
---

# 抬头

本文已在本期刊的未来期刊中被接受发表，但尚未完全编辑。 内容可能会在最终发布之前发生变化 引用信息：DOI 10.1109 / TC.2017.2709742，IEEE计算机上的交易

# 题目

路由还是计算？基于深度学习的范式转向智能化计算机网络分组传输

# 作者

Bomin Mao，学生会员，IEEE，Zubair Md.Fadlullah，高级会员，IEEE，Fengxiao Tang，IEEE学生会员，Nei Kato，IEEE院士，Osamu Akashi，Takeru Inoue和Kimihiro Mizutani

# 摘要

近年来，软件定义路由器（SDR）（可编程路由器）已成为一种可行的解决方案，可提供具有易扩展性和可编程性的经济高效的数据包处理平台。多核平台显着地推动了SDR的并行计算能力，使他们能够采用人工智能技术，即深度学习来管理路由路径。在本文中，我们通过深度学习探索数据包处理的新机会，以便将计算需求从基于规则的路由计算转移到基于深度学习的路由估计，以实现高吞吐量数据包处理。尽管深度学习技术已在各种计算领域得到广泛应用，但迄今为止，研究人员还未能有效地利用基于深度学习的路由计算来实现高速核心网络。我们设想一个有监督的深度学习系统来构建路由表，并展示如何使用中央处理单元（CPU）和图形处理单元（GPU）将所提出的方法与可编程路由器集成。我们演示了我们独特的输入和输出流量模式如何通过分析和广泛的计算机模拟来增强基于深度学习的SDR的路由计算。值得一提的是，仿真结果表明我们的方案在延迟、吞吐量和信令开销方面优于基准方法。

# 索引术语

软件定义路由器，网络流量控制，深度学习，骨干网络，核心网络，路由。

# 1. 简介

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;互联网核心中的路由器见证了硬件的几代变化，而路由算法背后的主要思想在传统意义上非常相似。这是因为互联网核心和有线/无线异构骨干网络的构建方式多年来基本保持不变[1]。另一方面，为了适应网络流量的巨大增长，互联网核心基础设施通过添加更多/更大的路由器和更多/更快的链路而继续扩展。越来越大的核心网络推动核心路由器的架构在计算和交换容量方面更加强大。即使最近数据流量激增，网络运营商也面临着流量管理的挑战，以确保服务质量（QoS）以及处理利润下降[2]。传统上，运营商依赖于硬件解决方案以便改善核心网络性能，即通过简单地增加路由器和/或链路的数量和大小，这导致大量的投资挥霍。另一方面，交通管理的软件方面主要关注新路由策略的应用，这些策略在新一代功能强大的硬件架构出现之前可能是不可能的[3]。换句话说，由于连续变化的网络环境，软件驱动的路由策略的进步似乎总是落后于流行的路由策略。为了应用为不同网络服务开发的最先进的软件驱动路由算法，有必要提高核心路由器的可编程性。由于其专有的硬件架构，设计可编程路由器是一个具有挑战性的研究领域。因此，研究人员已经考虑使用软件定义路由器（SDR），其在商用硬件架构上部署可编程路由策略以执行分组处理和传输。作为软件定义网络（SDN）的关键组件，SDR不仅需要支持基于软件的数据包传输，还需要根据网络运营商的要求灵活地执行其他功能，以优化其网络。然而，最近涉及SDR的研究工作主要集中在增强其硬件方面，以便提高与基于硬件的路由器相当的处理吞吐量性能。例如，研究人员和网络制造商已经探索了基于多核的SDR [4]。虽然多核中央处理器（CPU）在相关研究领域占主导地位[5]，但图形处理单元（GPU）——加速SDR正在成为一项激动人心的研究领域。 这背后的原因是GPU同时运行数万个线程以有效处理数据包的固有能力[3]，[6]，[7]。 换句话说，GPU可以执行相同的程序以并行方式处理不同的数据集，这些数据也可以与多核CPU结合以同时进行不同的指令[8]。 GPU和CPU的协作可以显着提升SDR的数据包处理吞吐量[7]，可以将其视为现代骨干网的主流候选者。

- B. Mao，Z. M. Fadlullah，F. Tang和N. Kato在日本仙台东北大学信息科学研究生院工作。
电子邮件：{fbomin.mao，zubair，fengxiao.tang & katog} @it.is.tohoku.ac.jp
- O. Akashi，T. Inoue和K. Mizutani与日本电报电话公司（NTT）网络创新实验室合作。
电子邮件：{fakashi.osam，inoue.takeru，mizutani.kimihirog} @lab.ntt.co.jp

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了进一步完善骨干路由器硬件架构的研究，还需要改进网络流量控制策略，以满足近几十年来不断增长的流量需求[9]。这是因为良好的路由路径管理技术可以直接缓解（甚至克服）网络拥塞。在本文中，我们将重点放在分组路由策略的设计上。根据最大或最小度量值选择路径的基于规则的传统路由策略（例如，最短路径（SP）算法等）通常归因于显着慢收敛的缺点。此外，它们可能不是特别适合于多度量网络，因为难以手动估计多个度量之间的关系[10]。为了利用各种指标之间的复杂关系来决定最佳路径，基于机器学习的智能网络流量控制系统在广泛的网络环境中引起了广泛关注[11]，[12]。已经利用监督、无监督和强化学习技术来管理许多不同网络场景中的分组路由[13] - [15]，例如无线网状网络（WMN）[14]。然而，由于传统机器学习技术在处理多个网络参数时的低效率以及输入和输出特征化的困难，这些智能策略仍然基于规则的传统路由[16]，[17]。自2006年以来，已经见证了在深度学习架构（例如，多层神经网络）中已经实现了逐步突破，使得它们能够适应曾经只有人类占据主导地位的极其复杂的活动，例如谷歌的AlphaGo在复杂的棋盘游戏中的表现[18]。此外，GPU已经成为运行深度学习算法的最可行的硬件平台，因为它们具有高度并行的计算能力，而且还有各种GPU制造商提供的大量编程工具包[8]。因此，通过利用GPU加速的SDR来利用深度学习算法来实现智能网络流量控制（例如，智能路由管理）是可行的。

<img src = "2019_01_04_01.png" width = 50%>
- 图1：最近的跨学科趋势表明涉及计算系统，计算机网络和机器智能的跨学科领域。 特别地，由于CPU / GPU技术的进步和深度学习，网络流量控制系统正变得健壮和智能

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们在本文中的工作贡献如下。首先，我们从三个方面探讨路由策略，即网络流量控制，深度学习和CPU / GPU计算架构，如图1所示。其次，我们提出了一种基于深度学习的GPU加速SDR路由表构建方法。在我们的提议中，我们采用有监督的深度信念架构（DBA）[19]来计算以边缘路由器的流量模式为输入的后续节点（即路由器）。第三，我们根据边缘路由器的传入流量模式，为我们采用的DBA提供输入和输出的独特特征。根据收集的数据训练所提出的DBA，所述数据包括入站流量模式和相应的后续节点（即，路由器）。此外，我们还演示了经过培训的DBA如何预测下一个节点。第四，我们展示了基于深度学习的路由策略在较低信令开销和快速收敛方面的优势，从而显着改善了流量控制。第五，我们通过分析和广泛的仿真结果证明了我们提出的基于深度学习的解决方案与基准路由方法相比的有效性。特别是，我们演示了我们的提案如何在GPU加速的SDR上工作，并在分析其复杂性后评估运行路由策略的时间成本。结果表明，所提出的路由策略在GPU上的运行速度比在CPU上快100多倍。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在本文的其余结构如下。 第2节包括相关的研究工作。在第3部分中，我们描述了我们提出的路由DBA结构及其在GPU加速SDR中的工作原理。然后，我们在第4节和第5节介绍基于深度学习的路由策略的三个阶段，分析我们提案的复杂性，并比较GPU和CPU的理论时间成本。我们的提案的网络性能评估在第6节中介绍。最后，第7节结束了本文。

# 2. 相关研究工作

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在本节中，我们将从硬件和软件角度介绍相关的研究工作，分别考虑SDR和深度学习的最新技术。 此外，在讨论深度学习相关研究的同时，我们还描述了文献中存在的相关机器智能路由策略。

## 2.1 SDR相关研究工作

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于深度学习执行需要高性能计算硬件的支持，因此有必要讨论最先进的SDR技术。尽管将数据包处理逻辑实施到硬件中的方法继续提高路由器的线路速率高达100 Gbps，但应用新的网络流量控制算法仍然是一个具有挑战性的问题，因为在可行性和可扩展性方面存在专用硬件的缺点[6],[20]。另一方面，基于通用硬件的SDR在分组处理中不是很有效，尽管它是可编程的和灵活的。由于多核解决方案可以显着提高计算能力，因此学术界和工业界的研究人员对利用CPU或GPU提供的多核和/或线程并行运行路由任务以提高处理吞吐量表现出极大的兴趣。目前的SDR [6]，[21]。[21]中介绍的RouteBricks架构探索了一种新颖的网络架构，其中数据包在运行在通用PC硬件集群上的软件中处理。RouteBricks的IPv4转发吞吐量已经显示为高达8.7 Gbps和64B数据包。但是，由于CPU成为更多计算密集型应用程序的瓶颈，其性能可能不会超过10 Gbps。作为解决此问题的方法，PacketShader架构[3]将并行数据包处理的计算需求从CPU转移到GPU，因为GPU与CPU相比包含更多内核。使用单个商品PC评估此体系结构表明PacketShader能够以39 Gbps转发64B IPv4数据包。此外，在[3]中指出，通过扩展I / O集线器的性能，线速率可以进一步提高到100 Gbps [7]。此外，包括英特尔和思科在内的网络制造商正在采用类似的项目来开发SDR架构中的高性能计算设备并开发商业产品[4]，[5]。从上述工作中可以注意到，当前的可编程SDR可以提供有竞争力的线路速率（即，与传统的专用设计路由器相比，费用要低得多），同时保持灵活性和可扩展性的强度。

## 2.2 深度学习与机器智能路由策略的相关研究工作

<img src = "2019_01_05_02.png" width = 50%>

- 图2：考虑系统模型和问题陈述。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于机器学习对于预测网络参数很有用，一些研究人员已经尝试利用人工神经网络（ANN）（一种机器学习技术）来预测网络流量[15]，链路带宽或其他指标[22]。超过几个时间间隔。然而，这些策略的效率受到传统机器学习技术的缺点的限制，主要是因为缺乏适当的深层结构学习算法，足够大的训练数据的可用性等等。实际上，根据[11]中的工作，尽管添加层可以在极其复杂的应用中提取更高级别的特征，但是包含许多隐藏层的传统深度神经网络架构表现出与浅层相比较差的性能。最近，深度学习成为一种有前途的计算模型，可以有效地利用多个处理层从多个抽象层次的数据中提取特征[11]。此方法使用通用非线性模块将一个级别的表示转换为更高和更抽象级别的表示，然后组合这些表示，通过它可以自动且准确地学习特征[23]。 2006年引入了Greedy Layer-Wise培训，利用无监督学习程序对深度神经网络体系结构进行预训练，彻底改变了深度学习技术[19]，[24]。通过Greedy LayerWise预训练，接着是基于反向传播算法的整个深层架构的微调，深度学习在语音识别等许多复杂应用中取得了创纪录的成果[25]等等。将深度学习应用于语音识别的一个很好的例子是Apple的智能助手叫做“Siri”[26]。在传统上由人类主导的其他领域，研究人员已经探索了深度学习的应用并取得了令人鼓舞的成果，例如谷歌的AlphaGo在非常复杂的棋盘游戏中[18]。从文献中可以明显看出，深度学习技术已成为通常需要大量计算的应用中的最新技术。一方面，深度学习应用仅限于图像/字符/模式识别和自然语言处理领域[11]。另一方面，尚未成功尝试对网络流量控制系统的深度学习，其仍继续经历不断增长的计算负担。这是因为连接到互联网的设备数量越来越多，全球网络流量在最近几十年中爆炸式增长[9]。因此，在设计路由策略以满足严格和不断变化的网络要求时，网络运营商需要考虑更多参数和更复杂的规则。根据[16]，深度学习具有很好的潜力，可以应用于网络流量控制系统，通过考虑各种要求来估计最佳路径。此外，由于将深度学习应用于路由仍然是一个非常新的主题，因此以前的研究工作都没有尝试将深度学习整合到SDR中。另一方面，我们的动机来自于SDR与深度学习密切相关的事实，因为许多SDR使用多核平台（例如GPU）。这为在SDR中采用基于深度学习的智能路由策略铺平了道路。从这一点来看，我们尝试通过采用GPU驱动的SDR来执行深度学习技术来估计下一个节点以实现更好的路由管理，从而考虑硬件和软件两个方面。

<img src = "2019_01_05_03.png" width = 50%>

- 图3：考虑提出的深度学习系统的模型。

# 3. 基于深度学习的路由策略设计

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在本节中，我们将介绍如何设计深度学习结构以在GPU加速的SDR上构建路由表。首先，我们提出深度学习结构的输入和输出的详细表征，然后我们描述我们选择的架构，DBA。 接下来，讨论所提出的路由表构造方法如何在GPU加速的SDR上工作。

## 3.1 输入和输出设计

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们考虑的核心网络系统模型在图2中描绘，其包括多个有线骨干路由器。值得注意的是，也可以考虑无线骨干网络。在所考虑的网络中，假设边缘路由器连接到不同类型的网络，例如蜂窝网络，WMN等。从后面的网络生成的数据包到达边缘路由器，并且发往其他边缘路由器以进行传送。另一方面，内部路由器只负责将数据包转发到适当的边缘路由器。传统上，每个路由器周期性地将信令分组转发到其他路由器以通知延迟值或其到其邻居的链路的一些其他度量。然后，每个路由器可以利用该信息来计算用于将数据分组发送到目的地路由器的下一个节点。这种方法在大多数情况下都能很好地工作，因为每个路由器都可以根据获得的所有网络链路的信息做出最佳决策。然而，当网络中的一些路由器由于压倒性的流量需求而拥挤时，计算下一个节点的传统方法遭受慢收敛。同时，周期性信令交换加剧了交通拥堵。此外，传统的路由方法无法处理网络环境持续变得更加复杂的场景，这需要网络运营商考虑各种不相关的参数来确定路由规则。由于深度学习方法已应用于许多复杂的活动以自动探索各种输入之间的关系，我们尝试在本节的其余部分采用深度学习进行路由。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于在每个路由器处观察到的流量模式直接指示该路由器的流量情况，因此我们采用流量模式作为深度学习模型的输入。如第1节所述，深度学习结构用于计算路由路径。因此，我们选择路由路径作为模型的输出。因此，图3（a）表明业务模式被用作深度学习结构的输入并被处理以作为输出的路由路径决定。然后，关键的挑战是表征深度学习结构的输入和输出。为了表征输入，我们使用每个路由器上的流量模式，可以定义为每个时间间隔内路由器的入站数据包数量，如图3（b）所示。如果我们假设计入入站数据包的时间间隔是$\Delta t$秒，那么对于每个路由器，我们可以采用最后一个$\beta \Delta t$（$\beta$为正整数）秒内每个时间间隔内的入站数据包数量。交通模式。因此，通过假设网络由$N$个路由器组成，我们可以使用$\beta$行和$N$列的矩阵来表示网络中所有路由器的流量模式，并将矩阵中的$\beta N$元素的值输入到输入层。深度学习结构。请注意，$\beta$的值不应该太大，因为很久以前的流量模式对当前网络分析没有影响。此外，如果$\beta$的值太大，则深度学习结构具有高复杂性和低效率。在我们提出的深度学习结构中，仿真结果表明，将$\beta$的值设置为1是足够准确的。因此，深度学习结构的输入可以看作是$N$维向量，其第$i^{th}$个元素是最后$\Delta t$秒内第$i^{th}$个路由器的流量模式。接下来，我们需要设计输出层。出于路由的目的，深度学习结构需要输出路由路径。因此，输出层可以设计为提供类似于集中式路由的整个路径，或者仅提供类似于分布式路由策略的下一个节点。由于其较低的复杂性和较高的容差，后者在我们的提案中被选择。对于由$N$个路由器组成的网络，我们使用由$N$个二进制元素组成的向量来表示输出。在向量中，只有一个元素的值为1，其顺序表示下一个节点。这意味着如果$N$维向量中的第$i^{th}$个元素是1，则选择所考虑的网络中的第$i^{th}$个路由器作为下一个节点。总之，我们可以使用两个$N$维向量$x$和$y$来表示深度学习结构的输入和输出，$x$和$y$的示例如下：

$$ x = (tp_1, tp_2, ..., tp_{N - 1}, tp_N), \tag{1}$$
$$ y = (0, 1, ..., 0, 0), \tag{2} $$

其中$tp_i$表示路由器$i$的流量模式，其由上一时间间隔中的入站数据包的数量来度量。 此外，在向量$y$中，我们可以发现$y_2 = 1$，这意味着选择路由器2作为下一个节点。 由于$y$的二进制值，深度学习结构是逻辑回归模型，我们需要接下来设计。

## 3.2 深度学习结构设计

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了设计深度学习结构，需要用于执行监督训练的标记数据（即，多组$(x, y)$）。 为了完成计算具有流量模式的下一个路由器的任务，我们选择图3（c）所示的DBA作为我们的深度学习结构，因为它在所有深度学习模型中最常见和有效[27]。 如图所示，我们假设DBA由L层，输入层，$x$，输出层，$y$和$(L - 2)$隐藏层组成。 DBA也可以看作是一堆$(L - 2)$受限玻尔兹曼机器（RBMs）和一个逻辑回归层作为顶层。 每个RBM的结构如图3（d）所示。可以看出，每个RBM由两层组成，可见层，$v$和隐藏层$h$。两层中的单元通过加权链路连接，而同一层中的单元未连接。应注意，对两层中的每个单元给予加权偏差。术语$w_{ji}$表示连接隐藏层中的单元$j$和可见层中的单元$i$的链路的重量。此外，$a_i$和$b_j$分别表示可见层中的单元$i$和隐藏层中的单元$j$的偏置。隐藏层中学习单元的激活值用作DBA中上RBM的“可见数据”。如第二节所述。 2.2，深度学习训练过程包括两个步骤：初始化结构的Greedy Layer-Wise训练和微调结构的反向传播过程。对于DBA，初始过程是训练每个RBM，这是一个无监督的学习过程，原因是RBM是一个无向图形模型，其中可见层中的单元使用对称加权连接连接到随机隐藏单元，如图所示.3（d）[28]。在训练RBM时，将未标记的数据集给予可见层，并且重复调整权重和偏差的值，直到隐藏层可以重建可见层。因此，训练后的隐藏层可以看作是可见层的抽象特征。训练RBM是使隐藏层最小化重建错误的过程。为了对训练过程进行数学建模，我们使用如下给出的可见层的对数似然函数。然后，训练过程是更新权重和偏差的值以最大化对数似然函数的值。

$$ l(\theta, a) = \sum_{t = 1}^{m} log p(v^{(t)}) \tag{3} $$

其中$\theta$表示由隐藏层的权重和偏差的所有值组成的向量。 $\theta$可写为$\theta = (w, b)$。$w$和$b$分别表示由隐藏单元$b_j$的所有权重，$w_{ji}$和偏差组成的向量。 $a$由可见单元的偏差组成，$a_i$。 $m$表示训练数据的数量。 $v^{(t)}$是$t^{th}$训练数据，其概率为$p(v^{(t)})$。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了最大化$l(\theta, a)$，我们可以使用$l(\theta, a)$的梯度下降来调整$w$，$a$和$b$，其可以在等式4和5中描述。

$$ \theta := \theta + \eta\frac{\partial l(\theta, a)}{\partial \theta} \tag{4} $$

$$ a_i := a_i + \eta\frac{\partial l(\theta, a)}{\partial a_i} \tag{5} $$

其中$\eta$是深度学习的学习率。 这里，$\theta$表示任何$w_{ji}$和任何$b_j$。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了计算$p(v)$的值（代表任何$p(v^{(t)})$），我们需要将RBM建模为能量模型，因为RBM是对数线性马尔可夫随机场（MRF）的特定形式[29]。 能量函数$E(v, h)$和联合概率函数$p(v, h)$定义如下。

$$ E(v, h) = -\sum_{i} a_i v_i - \sum_{j} b_j h_j - \sum_{i}\sum_{j}h_j w_{ji} v_i \tag{6} $$

$$ p(v, h) = \frac{e^{-E(v, h)}}{Z} \tag{7} $$

$$ Z = \sum_{v} \sum_{h} e^{-E(v, h)} \tag{8} $$

其中$v_i$和$h_j$分别是图3（d）所示的可见层中的单元$i$和隐藏层中的单元$j$。 $Z$表示归一化常数分区函数。 此外，$p(v)$和$p(v, h)$之间的关系可以表示如下。

$$ p(v) = \sum_{h} p(v, h) \tag{9} $$

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们可以使用等式3到9来获得$θ$[28]的值。然而，等式8中$\sum_{v} \sum_{h}$的计算的复杂度是$2^{n_v + n_h}$，其非常高（$n_v$和$n_h$分别表示向量$v$和$h$的维度）。另一个问题是，为了计算等式8，有必要但不可能考虑$v$和$h$的所有可能值而不是仅考虑所获得的训练数据。为了解决这些问题，Hinton等人。提出了对比分歧（CD）方法[30]。 CD的主要思想是使用吉布斯采样方法对v和h的值进行采样以逼近实际值，因为一层的条件分布概率（同时给出了另一层的值），例如$p(v | h; θ, a)$，可以计算出来。文中省略了CD的详细程序，可以在[30]中找到。由于每个单元的值独立于同一层中的其他单元，当一个层固定时，另一层的条件分布概率可以如下计算，

$$ p(v | h; θ, a) = \prod_i p(v_i | h; θ, a) \tag{10} $$

$$ p(h | v; θ, a) = \prod_j p(h_j | v; θ, a) \tag{11} $$

其中$p(v | h; θ, a)$和$p(h | v; θ, a)$分别是给定h的v的条件概率和给定$v$的$h$的条件概率。$p(v_i | h; θ, a)$是隐藏层固定时可见层中单元$i$的条件概率分布。 此外，$p(h_j | v; θ, a)$是当可见层固定时隐藏层中单元$j$的条件概率分布。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果可见层和隐藏层中的单元的值都是二进制的，则$p(v_i = 1 | h; θ, a)$和$p(h_j = 1 | v; θ, a)$如下给出。

$$ p(v_i = 1 | h; θ, a) = sigm(\sum_{j} w_{ji} h_j + a_i) \tag{12} $$

$$ p(h_j = 1 | v; θ, a) = sigm(\sum_{i} w_{ji} v_i + b_j) \tag{13} $$

# 7. 总结

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在本文中，我们解释了重新思考核心路由器架构和骨干网络路由策略的重要性，以满足不断变化的网络需求并应对未来几天的大量流量增长。在这种情况下，我们探索了当前的SDR架构，并设想深度学习（最近作为一种有前途的机器学习技术出现）可用于计算路由路径而不是传统的路由协议。这可以大大改善骨干网络流量控制。考虑到当前GPU加速的SDR实现了大规模并行计算，我们提出了一种监督深度学习系统来利用流量模式直接计算路径，这与传统的基于规则的路由不同。仿真结果表明，所提出的基于深度学习的路由策略在网络分组传输吞吐量和每跳平均延迟方面优于传统OSPF，因为我们的提议具有低得多的信令开销。这表明路由计算从传统的基于规则的策略向深度学习的转变可以大大改善骨干网络控制。此外，我们分析了我们提出的路由策略的复杂性，以评估GPU加速的SDR比基于CPU的SDR更有效地运行所提出的算法。由于各种终端接入网络以获得不同类型的服务，我们未来的研究将尝试应用深度学习技术来建模多个网络指标之间的复杂关系，以便更好地进行路由路径管理